{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\__init__.py:29: UserWarning: loaded more than 1 DLL from .libs:\n",
      "E:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.IPBC74C7KURV7CB2PKT5Z5FNR3SIBV4J.gfortran-win_amd64.dll\n",
      "E:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From E:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 17s 17ms/step - loss: 7.5138 - acc: 0.5200\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 7.9871 - acc: 0.4990\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 7.9871 - acc: 0.4990\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 7.9871 - acc: 0.4990\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 7.9871 - acc: 0.4990\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 7.9871 - acc: 0.4990\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 7.9871 - acc: 0.4990\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 7.9871 - acc: 0.4990\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 7.9871 - acc: 0.4990\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 7.9871 - acc: 0.4990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4d6dc88d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "weishu=978\n",
    "\n",
    "yaowu=np.random.randint(1,100,size=(1000,weishu))\n",
    "babiao=np.random.randint(1,100,size=(1000,weishu))\n",
    "input1=np.concatenate((yaowu, babiao),axis=1)\n",
    "#input2=np.stack((yaowu, babiao), axis=1)\n",
    "output=np.random.randint(0,2,size=(1000))\n",
    "\n",
    "\n",
    "wide_input=Input(shape=(input1.shape[1],),name='wide')\n",
    "\n",
    "final_output=layers.Dense(200)(wide_input)\n",
    "final_output=layers.Dense(10)(final_output)\n",
    "final_output=layers.Dense(1,activation = 'sigmoid')(final_output)\n",
    "\n",
    "model = Model(wide_input,final_output)\n",
    "\n",
    "model.compile(optimizer = 'rmsprop',loss = 'binary_crossentropy',metrics = ['acc'])\n",
    "\n",
    "model.fit(input1,output,epochs = 10,batch_size = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score ,f1_score,roc_auc_score\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "from keras import regularizers\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data=np.load(r'after_merge/A375/pos_data.npy')\n",
    "neg_data=np.load(r'after_merge/A375/neg_data.npy')\n",
    "\n",
    "use_neg=random.sample(list(neg_data),2*len(pos_data))\n",
    "use_neg=np.array(use_neg)\n",
    "\n",
    "use_data=[]\n",
    "\n",
    "for d in pos_data:\n",
    "    use_data.append((d,1))\n",
    "\n",
    "for d in use_neg:\n",
    "    use_data.append((d,0))\n",
    "\n",
    "random.shuffle(use_data)   \n",
    "\n",
    "x,y=zip(*use_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=np.array(x)\n",
    "train_targets=np.array(y)\n",
    "#10折交叉验证\n",
    "k = 10\n",
    "num_val_samples = len(train_data) // k\n",
    "num_epochs = 100\n",
    "all_acc = []\n",
    "all_auc = []\n",
    "all_f1 = []\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    # 验证集\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # 训练集\n",
    "    partial_train_data = np.concatenate(\n",
    "        [train_data[:i * num_val_samples],\n",
    "         train_data[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [train_targets[:i * num_val_samples],\n",
    "         train_targets[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    \n",
    "    #训练数据\n",
    "    input1=partial_train_data.reshape(partial_train_data.shape[0],partial_train_data.shape[1]*partial_train_data.shape[2])\n",
    "    input2=partial_train_data\n",
    "    output=partial_train_targets\n",
    "    #测试数据\n",
    "    test_input1=val_data.reshape(val_data.shape[0],val_data.shape[1]*val_data.shape[2])\n",
    "    test_input2=val_data\n",
    "    test_output=val_targets\n",
    "    \n",
    "    #noredim-3layersdnn\n",
    "    wide_input=Input(shape=(input1.shape[1],),name='wide')\n",
    "\n",
    "    final_output=layers.Dense(200)(wide_input)\n",
    "    final_output=layers.Dense(10)(final_output)\n",
    "    final_output=layers.Dense(1,activation = 'sigmoid')(final_output)\n",
    "\n",
    "    model = Model(wide_input,final_output)\n",
    "    \n",
    "    model.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics = ['acc'])\n",
    "    \n",
    "    model.fit(input1,output,epochs = 100,batch_size = 64)\n",
    "    \n",
    "    pred=model.predict(test_input1)\n",
    "    for pred_i in pred:\n",
    "        pred_i[pred_i >=0.5] = 1\n",
    "        pred_i[pred_i < 0.5] = 0\n",
    "    all_f1.append(f1_score(test_output, pred.flatten(), average='micro'))\n",
    "    all_auc.append(roc_auc_score(test_output, pred.flatten()))\n",
    "    all_acc.append(accuracy_score(test_output, pred.flatten(),normalize =True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dim(wide_dim,deep_dim):  \n",
    "    train_data=np.array(x)\n",
    "    train_targets=np.array(y)\n",
    "    #10折交叉验证\n",
    "    k = 10\n",
    "    num_val_samples = len(train_data) // k\n",
    "    num_epochs = 100\n",
    "    all_acc = []\n",
    "    all_auc = []\n",
    "    all_f1 = []\n",
    "    for i in range(k):\n",
    "        print('processing fold #', i)\n",
    "        # 验证集\n",
    "        val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "        val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "        # 训练集\n",
    "        partial_train_data = np.concatenate(\n",
    "            [train_data[:i * num_val_samples],\n",
    "             train_data[(i + 1) * num_val_samples:]],\n",
    "            axis=0)\n",
    "        partial_train_targets = np.concatenate(\n",
    "            [train_targets[:i * num_val_samples],\n",
    "             train_targets[(i + 1) * num_val_samples:]],\n",
    "            axis=0)\n",
    "\n",
    "        #训练数据\n",
    "        input1=partial_train_data.reshape(partial_train_data.shape[0],partial_train_data.shape[1]*partial_train_data.shape[2])\n",
    "        input2=partial_train_data\n",
    "        output=partial_train_targets\n",
    "        #测试数据\n",
    "        test_input1=val_data.reshape(val_data.shape[0],val_data.shape[1]*val_data.shape[2])\n",
    "        test_input2=val_data\n",
    "        test_output=val_targets\n",
    "\n",
    "        #redim-use wad and 3 layers dnn\n",
    "        wide_input=Input(shape=(input1.shape[1],),name='wide')\n",
    "        #wide_dim\n",
    "        wide_output=layers.Dense(wide_dim)(wide_input)\n",
    "\n",
    "        deep_input=Input(shape=(input2.shape[1],input2.shape[2],),name='deep')\n",
    "        deep_hide=layers.LSTM(400,return_sequences=True)(deep_input)\n",
    "        deep_hide=layers.LSTM(400)(deep_hide)\n",
    "        #deep_dim\n",
    "        deep_output=layers.Dense(deep_dim)(deep_hide)\n",
    "\n",
    "        #deep_dim+wide_dim\n",
    "        concatenated = layers.concatenate([wide_output,deep_output],axis = -1)\n",
    "\n",
    "        #3dnn\n",
    "        final_output=layers.Dense(200)(concatenated)\n",
    "        final_output=layers.Dense(10)(final_output)\n",
    "        final_output=layers.Dense(1,activation = 'sigmoid')(final_output)\n",
    "\n",
    "        model = Model([wide_input,deep_input],final_output)\n",
    "\n",
    "        model.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics = ['acc'])\n",
    "\n",
    "        model.fit([input1,input2],output,epochs = 100,batch_size = 64)\n",
    "\n",
    "        pred=model.predict(test_input1)\n",
    "        for pred_i in pred:\n",
    "            pred_i[pred_i >=0.5] = 1\n",
    "            pred_i[pred_i < 0.5] = 0\n",
    "        all_f1.append(f1_score(test_output, pred.flatten(), average='micro'))\n",
    "        all_auc.append(roc_auc_score(test_output, pred.flatten()))\n",
    "        all_acc.append(accuracy_score(test_output, pred.flatten(),normalize =True))\n",
    "        return np.mean(all_acc),np.mean(all_auc),np.mean(all_f1)\n",
    "\n",
    "wide_dim=[50,100,150,200,250,300]\n",
    "deep_dim=[100,200,300,400]\n",
    "all_value=[]\n",
    "for w in wide_dim:\n",
    "    for d in deep_dim:\n",
    "        the_acc,the_auc,the_f1=reduce_dim(w,d)\n",
    "        all_value.append([w,d,the_acc,the_auc,the_f1])\n",
    "        print(\"wide_dim:\",w,\"deep_dim:\",d)\n",
    "        print(\"acc:\",the_acc,\"auc:\",the_auc,\"f1:\",the_f1)\n",
    "for v in all_value:\n",
    "    print(\"wide_dim:\",v[0],\"deep_dim:\",v[1])\n",
    "    print(\"acc:\",v[2],\"auc:\",v[3],\"f1:\",v[4])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_value=[]\n",
    "a=1\n",
    "b=2\n",
    "c=3\n",
    "all_value.append([a,b,c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
